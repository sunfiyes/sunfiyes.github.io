---
layout: post
title: "小时到分钟 - 一步步优化巨量关键词的匹配"
date: 2017-07-11 18:00:08 +0800
comments: true
---

<h1 id="toc_0">问题由来</h1>
<p>前些天工作中遇到一个问题：</p>
<blockquote>
    <p>有 60万 条短消息记录日志，每条约 50 字，5万 关键词，长度 2-8 字，绝大部分为中文。要求将这 60万 条记录中包含的关键词全部提取出来并统计各关键词的命中次数。</p>
</blockquote>
<p>本文完整介绍了我的实现方式，看我如何将需要运行十小时的任务优化到十分钟以内。虽然实现语言是 PHP，但本文介绍的更多的思想，应该能给大家一些帮助。</p>
<hr />
<h1 id="toc_1">原始 - grep</h1>
<h3 id="toc_2">设计</h3>
<p>一开始接到任务的时候，我的小心思立刻转了起来，<code>日志 + 关键词 + 统计</code>，我没有想到自己写代码实现，而是首先想到了 linux 下常用的日志统计命令 <code>grep</code>。</p>
<p><code>grep</code>命令的用法不再多提，使用 <code>grep 'keyword' | wc -l</code> 可以很方便地进行统计关键词命中的信息条数，而php的 <code>exec()</code> 函数允许我们直接调用 linux 的 shell 命令，虽然这样执行危险命令时会有安全隐患。</p>
<h3 id="toc_3">代码</h3>
<p>上伪代码：</p>
<div>
<pre class="line-numbers"><code class="language-none">foreach ($word_list as $keyword) {
    $count = intval(exec("grep '{$keyword}' file.log | wc -l"));
    record($keyword, $count);
}</code></pre>
</div>
<p>在一台老机器上跑的，话说老机器效率真的差，跑了6小时。估计最新机器2-3小时吧，后面的优化都使用的新机器，而且需求又有变动，正文才刚刚开始。</p>
<p><em>原始，原始在想法和方法</em>。</p>
<hr />
<h1 id="toc_4">进化 - 正则</h1>
<h3 id="toc_5">设计</h3>
<p>交了差之后，第二天产品又提出了新的想法，说以后想把某数据源接入进来，消息以数据流的形式传递，而不再是文件了。而且还要求了消息统计的实时性，一下把我想把数据写到文件再统计的想法也推翻了，为了方案的可扩展性，现在的统计对象不再是一个整体，而是要考虑拿n个单条的消息来匹配了。</p>
<p>这时，略懵的我只好祭出了最传统的工具- <code>正则</code>。正则的实现也不难，各个语言也都封装好了正则匹配函数，重点是模式(pattern)的构建。</p>
<p>当然这里的模式构建也不难，<code>/keyword1|keword2|.../</code>，用<code>|</code>将关键词连接起来即可。</p>
<h3 id="toc_6">正则小坑</h3>
<p>这里介绍两个使用中遇到的小坑：</p>
<ul>
    <li>
        <p>正则模式长度太长导致匹配失败：</p>
        <p>PHP 的正则有回溯限制，以防止消耗掉所有的进程可用堆栈, 最终导致 php 崩溃。太长的模式会导致 PHP 检测到回溯过多，中断匹配，经测试默认设置时最大模式长度为 32000 字节 左右。</p>
        <p>php.ini 内 <code>pcre.backtrack_limit</code> 参数为最大回溯次数限制，默认值为 1000000，修改或<code>php.ini</code> 或在脚本开始时使用<code>ini_set(&lsquo;pcre.backtrack_limit&rsquo;, n);</code> 将其设置为一个较大的数可以提高单次匹配最大模式长度。当然也可以将关键词分批统计（我用了这个=_=）。</p>
    </li>
    <li>
        <p>模式中含有特殊字符导致大量warning：</p>
        <p>匹配过程中发现 PHP 报出大量 warning：<code>unknown modifier 乱码</code>，仔细检查发现关键词中有<code>/</code>字符，可以使用<code>preg_quote()</code>函数过滤一遍关键词即可。</p>
    </li>
</ul>
<h3 id="toc_7">代码</h3>
<p>上伪代码：</p>
<div>
<pre class="line-numbers"><code class="language-none">$end = 0;
$step = 1500;
$pattern = array();
// 先将pattern 拆成多个小块
while ($end &lt; count($word_list)) {
    $tmp_arr = array_slice($word_list, $end, $step);
    $end += $step;
    $item = implode('|', $tmp_arr);
    $pattern[] = preg_quote($item);
}

$content = file_get_contents($log_file);
$lines = explode("\n", $content);
foreach ($lines as $line) {
    // 使用各小块pattern分别匹配
    for ($i = 0; $i &lt; count($pattern); $i++) {
        preg_match_all("/{$pattern[$i]}/", $line, $match);
    }
    $match = array_unique(array_filter($match));
    dealResult($match);
}</code></pre>
</div>
<p>为了完成任务，硬着头皮进程跑了一夜。当第二天我发现跑了近十个小时的时候内心是崩溃的。。。太慢了，完全达不到使用要求，这时，我已经开始考虑改换方法了。</p>
<p>当产品又改换了关键词策略，替换了一些关键词，要求重新运行一遍，并表示还会继续优化关键词时，我完全否定了现有方案。绝对不能用关键词去匹配信息，这样一条一条用全部关键词去匹配，效率实在是不可忍受。</p>
<p><em>进化，需求和实现的进化</em></p>
<hr />
<h1 id="toc_8">觉醒 - 拆词</h1>
<h3 id="toc_9">设计</h3>
<p>我终于开始意识到要拿信息去关键词里对比。如果我用关键词为键建立一个 hash 表，用信息里的词去 hash 表里查找，如果查到就认为匹配命中，这样不是能达到 O(1) 的效率了么？</p>
<p>可是一条短消息，我如何把它拆分为刚好的词去匹配呢，分词？分词也是需要时间的，而且我的关键词都是些无语义的词，构建词库、使用分词工具又是很大的问题，最终我想到 <code>拆词</code>。</p>
<p>为什么叫拆词呢，我考虑以蛮力将一句话拆分为<code>所有可能的</code>词。如<code>我是好人</code>就可以拆成 <code>我是、是好、好人、我是好、是好人、我是好人</code>等词，我的关键词长度为 2-8，所以可拆词个数会随着句子长度迅速增加。不过，可以用标点符号、空格、语气词（如<code>的、是</code>等）作为分隔将句子拆成小短语再进行拆词，会大大减少拆出的词量。</p>
<p>其实分词并没有完整实现就被后一个方法替代了，只是一个极具实现可能的构想，写这篇文章时用伪代码实现了一下，供大家参考，即使不用在匹配关键词，用在其他地方也是有可能的。</p>
<h3 id="toc_10">代码</h3>
<div>
<pre class="line-numbers"><code class="language-none">$str_list = getStrList($msg);
foreach ($str_list as $str) {
    $keywords = getKeywords($str);
    foreach ($keywords as $keyword) {
        // 直接通过PHP数组的哈希实现来进行快速查找
        if (isset($word_list[$keyword])) {
            record($keyword);
        }
    }
}
/**
 * 从消息中拆出短句子
 */
function getStrList($msg) {
    $str_list = array();
    $seperators = array('，', '。', '的', ...);

    $words = preg_split('/(?&lt;!^)(?!$)/u', $msg);
    $str = array();
    foreach ($words as $word) {
        if (in_array($word, $seperators)) {
            $str_list[] = $str;
            $str = array();
        } else {
            $str[] = $word;
        }
    }

    return array_filter($str_list);
}

/**
 * 从短句中取出各个词
 */
function getKeywords($str) {
    if (count($str) &lt; 2) {
        return array();
    }

    $keywords = array();
    for ($i = 0; $i &lt; count($str); $i++) {
        for ($j = 2; $j &lt; 9; $j++) {
            $keywords[] = array_slice($str, $i, $j); // todo 限制一下不要超过数组最大长度
        }
    }

    return $keywords;
}</code></pre>
</div>
<h3 id="toc_11">结果</h3>
<p>我们知道一个 <code>utf-8</code> 的中文字符要占用三个字节，为了拆分出包含中英文的每一个字符，使用简单的 <code>split()</code> 函数是做不到的。</p>
<p>这里使用了 <code>preg_split('/(?&lt;!^)(?!$)/u', $msg)</code> 是通过正则匹配到两个字符之间的<code>''</code>来将两个字符拆散，而两个括号里的 <code>(?&lt;!^)(?!$)</code> 是分别用来限定捕获组不是第一个，也不是最后一个（不使用这两个捕获组限定符也是可以的，直接使用<code>//</code>作为模式会导致拆分结果在前后各多出一个空字符串项）。 捕获组的概念和用法可见我之前的博客 <a href="{{ site.baseurl }}/2015/11/php_regex_capture_group.html">PHP正则中的捕获组与非捕获组</a></p>
<p>由于没有真正实现，也不知道效率如何。估算每个短句长度约为 10 字左右时，每条短消息约50字左右，会拆出 200 个词。虽然它会拆出很多无意义的词，但我相信效率绝不会低，由于其 hash 的高效率，甚至我觉得会可能比终极方法效率要高。</p>
<p>最终没有使用此方案是因为它对句子要求较高，拆词时的分隔符也不好确定，最重要的是它不够优雅。。。这个方法我不太想去实现，统计标识和语气词等活显得略为笨重，而且感觉拆出很多无意义的词感觉效率浪费得厉害。</p>
<p><em>觉醒，意识和思路的觉醒</em></p>
<hr />
<h1 id="toc_12">终级 - Trie树</h1>
<h3 id="toc_13">trie树</h3>
<p>于是我又来找谷哥帮忙了，搜索大量数据匹配，有人提出了 使用 trie 树的方式，没想到刚学习的 trie 树的就派上了用场。我上上篇文章刚介绍了 trie 树，在<a href="{{ site.baseurl }}/2017/06/spatial_index_quad_tree.html">空间索引 - 四叉树</a> 里<code>字典树</code>这一小节，大家可以查看一下。</p>
<p>当然也为懒人复制了一遍我当时的解释(看过的可以跳过这一小节了)。</p>
<blockquote>
    <p>字典树，又称前缀树或 trie 树，是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。</p>
</blockquote>
<p>我们可以类比字典的特性：我们在字典里通过拼音查找晃(<code>huang</code>)这个字的时候，我们会发现它的附近都是读音为<code>huang</code>的，可能是声调有区别，再往前翻，我们会看到读音前缀为<code>huan</code>的字，再往前，是读音前缀为<code>hua</code>的字... 取它们的读音前缀分别为 <code>h qu hua huan huang</code>。我们在查找时，根据 <code>abc...xyz</code> 的顺序找到<code>h</code>前缀的部分，再根据 <code>ha he hu</code>找到 <code>hu</code> 前缀的部分...最后找到 <code>huang</code>，我们会发现，越往后其读音前缀越长，查找也越精确，这种类似于字典的树结构就是字典树，也是前缀树。</p>
<h3 id="toc_14">设计</h3>
<p>那么 trie 树怎么实现关键字的匹配呢? 这里以一幅图来讲解 trie 树匹配的过程。</p>
<p><img src="/images/2017/819496-20170717202442035-340893557.png" alt="" /></p>
<p>其中要点：</p>
<h4 id="toc_15">构造trie树</h4>
<ol>
    <li>将关键词用上面介绍的<code>preg_split()</code>函数拆分为单个字符。如<code>科学家</code>就拆分为<code>科、学、家</code>三个字符。</li>
    <li>在最后一个字符后添加一个特殊字符 <code>`</code>，此字符作为一个关键词的结尾（图中的粉红三角），以此字符来标识查到了一个关键词（不然，我们不知道匹配到<code>科、学</code>两个字符时算不算匹配成功）。</li>
    <li>检查根部是否有第一个字符(<code>科</code>)节点，如果有了此节点，到<code>步骤4</code>。 如果还没有，在根部添加值为<code>科</code>的节点。</li>
    <li>依次检查并添加<code>学、家</code> 两个节点。</li>
    <li>在结尾添加<code>`</code>节点，并继续下一个关键词的插入。</li>
</ol>
<h4 id="toc_16">匹配</h4>
<p>然后我们以 <code>这位科学家很了不起！</code>为例来发起匹配。</p>
<ul>
    <li>首先我们将句子拆分为单个字符 <code>这、位、...</code>；</li>
    <li>从根查询第一个字符<code>这</code>，并没有以这个字符开头的关键词，将字符&ldquo;指针&rdquo;向后移，直到找到根下有的字符节点<code>科</code>;</li>
    <li>接着在节点<code>科</code>下寻找值为 <code>学</code>节点，找到时，结果子树的深度已经到了2，关键词的最短长度是2，此时需要在<code>学</code>结点下查找是否有<code>`</code>，找到意味着匹配成功，返回关键词，并将字符&ldquo;指针&rdquo;后移，如果找不到则继续在此结点下寻找下一个字符。</li>
    <li>如此遍历，直到最后，返回所有匹配结果。</li>
</ul>
<h3 id="toc_17">代码</h3>
<p>完整代码我已经放到了GitHub上：<a href="https://github.com/zhenbianshu/DataStructureAndAlgorithm/blob/master/keyword_match_trie.php">Trie-GitHub-zhenbianshu</a>，这里放上核心。</p>
<p>首先是数据结构树结点的设计，当然它也是重中之重：</p>
<div>
<pre class="line-numbers"><code class="language-none"> $node = array(
    'depth' =&gt; $depth, // 深度，用以判断已命中的字数
    'next' =&gt; array(
        $val =&gt; $node, // 这里借用php数组的哈希底层实现，加速子结点的查找
        ...
    ),
);</code></pre>
</div>
<p>然后是树构建时子结点的插入：</p>
<div>
<pre class="line-numbers"><code class="language-none">// 这里要往节点内插入子节点，所以将它以引用方式传入
private function insert(&amp;$node, $words) {
         if (empty($words)) {
            return;
        }
        $word = array_shift($words);
        // 如果子结点已存在，向子结点内继续插入
        if (isset($node['next'][$word])) {
            $this-&gt;insert($node['next'][$word], $words);
        } else {
            // 子结点不存在时，构造子结点插入结果
            $tmp_node = array(
                'depth' =&gt; $node['depth'] + 1,
                'next' =&gt; array(),
            );
            $node['next'][$word] = $tmp_node;
            $this-&gt;insert($node['next'][$word], $words);
        }
    }</code></pre>
</div>
<p>最后是查询时的操作：</p>
<div>
<pre class="line-numbers"><code class="language-none">// 这里也可以使用一个全局变量来存储已匹配到的字符，以替换$matched
private function query($node, $words, &amp;$matched) {
        $word = array_shift($words);
        if (isset($node['next'][$word])) {
            // 如果存在对应子结点，将它放到结果集里
            array_push($matched, $word);
            // 深度到达最短关键词时，即可判断是否到词尾了
            if ($node['next'] &gt; 1 &amp;&amp; isset($node['next'][$word]['next']['`'])) {
                return true;
            }
            return $this-&gt;query($node['next'][$word], $words, $matched);
        } else {
            $matched = array();
            return false;
        }
    }</code></pre>
</div>
<h3 id="toc_18">结果</h3>
<p>结果当然是喜人的，如此匹配，处理一千条数据只需要3秒左右。找了 Java 的同事试了下，Java 处理一千条数据只需要1秒。</p>
<p>这里来分析一下为什么这种方法这么快：</p>
<ul>
    <li>
        <p>正则匹配：要用所有的关键词去信息里匹配匹配次数是 <code>key_len * msg_len</code>，当然正则会进行优化，但基础这样，再优化效率可想而知。</p>
    </li>
    <li>
        <p>而 trie 树效率最差的时候是 <code>msg_len * 9(最长关键词长度 + 1个特殊字符)</code>次 hash 查找，即最长关键词类似 <code>AAA</code>，信息内容为 <code>AAA...</code>时，而这种情况的概率可想而知。</p>
    </li>
</ul>
<p>至此方法的优化到此结束，从每秒钟匹配 10 个，到 300 个，30 倍的性能提升还是巨大的。</p>
<p><em>终级，却不一定是终极</em></p>
<hr />
<h1 id="toc_19">他径 - 多进程</h1>
<h3 id="toc_20">设计</h3>
<p>匹配方法的优化结束了，开头说的优化到十分钟以内的目标还没有实现，这时候就要考虑一些其他方法了。</p>
<p>我们一提到高效，必然想到的是 <code>并发</code>，那么接下来的优化就要从并发说起。PHP 是单线程的（虽然也有不好用的多线程扩展），这没啥好的解决办法，并发方向只好从多进程进行了。</p>
<p>那么一个日志文件，用多个进程怎么读呢？这里当然也提供几个方案：</p>
<ul>
    <li>
        <p>进程内添加日志行数计数器，各个进程支持传入参数 n，进程只处理第 <code>行数 % n = n</code> 的日志，这种 hack 的反向分布式我已经用得很熟练了，哈哈。</p>
        <p>这种方法需要进程传参数，还需要每个进程都分配读取整个日志的的内存，而且也不够优雅。</p>
    </li>
    <li>
        <p>使用 linux 的 <code>split -l n file.log output_pre</code> 命令，将文件分割为每份为 n 行的文件，然后用多个进程去读取多个文件。</p>
        <p>此方法的缺点就是不灵活，想换一下进程数时需要重新切分文件。</p>
    </li>
    <li>
        <p>使用 Redis 的 list 队列临时存储日志，开启多个进程消费队列。</p>
        <p>此方法需要另外向 Redis 内写入数据，多了一个步骤，但它扩展灵活，而且代码简单优雅。</p>
    </li>
</ul>
<p>最终使用了第三种方式来进行。</p>
<h3 id="toc_21">结果</h3>
<p>这种方式虽然也会有瓶颈，最后应该会落在 Redis 的网络 IO 上。我也没有闲心开 n 个进程去挑战公司 Redis 的性能，运行 10 个进程三四分钟就完成了统计。即使再加上 Redis 写入的耗时，10分钟以内也妥妥的。</p>
<p>一开始产品对匹配速度已经有了小时级的定位了，当我 10 分钟就拿出了新的日志匹配结果，看到产品惊讶的表情，心里也是略爽的，哈哈~</p>
<p><em>他径，也能帮你走得更远</em></p>
<hr />
<h1 id="toc_22">总结</h1>
<p>解决问题的方法有很多种，我认为在解决各种问题之前，要了解很多种知识，即使只知道它的作用。就像一个工具架，你要先把工具尽量摆得多，才能在遇到问题时选取一个最合适的。接着当然要把这些工具用是纯熟了，这样才能使用它们去解决一些怪异问题。</p>
<p>工欲善其事，必先利其器，要想解决性能问题，掌握系统级的方法还略显不够，有时候换一种数据结构或算法，效果可能会更好。感觉自己在这方面还略显薄弱，慢慢加强吧，各位也共勉。</p>
<p>有什么问题可以在下面留言交流，如果您觉得本文对您有帮助，可以点击下面的 <strong>推荐</strong> 支持一下我。博客一直在更新，欢迎 <strong>关注</strong> 。</p>